{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project root directory\n",
    "DATASETS_DIR = Path('../datasets/raw/toxic_comment_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read toxic comment datasets\n",
    "train_df = pd.read_csv(DATASETS_DIR / 'train.csv')\n",
    "test_df = pd.read_csv(DATASETS_DIR / 'test.csv')\n",
    "test_labels_df = pd.read_csv(DATASETS_DIR / 'test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.898321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "      <td>0.302226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate           none  \n",
       "count  159571.000000  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805       0.898321  \n",
       "std         0.216627       0.093420       0.302226  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       1.000000  \n",
       "50%         0.000000       0.000000       1.000000  \n",
       "75%         0.000000       0.000000       1.000000  \n",
       "max         1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxicity_types = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_df['none'] = 1-train_df[toxicity_types].max(axis=1)\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 9)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we have done EDA in 03_toxic_comment_eda.ipynb we will directly build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer + Naive Bayes Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text.lower())\n",
    "    return text.split()\n",
    "\n",
    "train_data, test_data = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IIT-J\\2nd year\\ML with big data\\Project\\youtube_comment_analysis\\yca-venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "count_vectorizer = CountVectorizer(tokenizer=tokenize, ngram_range=(1, 2), min_df=3)\n",
    "train_count = count_vectorizer.fit_transform(train_data['comment_text'])\n",
    "test_count = count_vectorizer.transform(test_data['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Approach (Count Vectorizer + Naive Bayes) Results: {'toxic': {'accuracy': 0.9362995456681811, 'f1': 0.6895709268590624, 'precision': 0.6464357286000573, 'recall': 0.7388743455497382}, 'severe_toxic': {'accuracy': 0.9853987153376156, 'f1': 0.44258373205741625, 'precision': 0.3592233009708738, 'recall': 0.5763239875389408}, 'obscene': {'accuracy': 0.9610527964906783, 'f1': 0.6657703683785964, 'precision': 0.6177644710578842, 'recall': 0.721865889212828}, 'threat': {'accuracy': 0.996960676797744, 'f1': 0.18487394957983194, 'precision': 0.24444444444444444, 'recall': 0.14864864864864866}, 'insult': {'accuracy': 0.9584208052639824, 'f1': 0.6237595690388432, 'precision': 0.575013068478829, 'recall': 0.6815365551425031}, 'identity_hate': {'accuracy': 0.9871533761554128, 'f1': 0.23220973782771537, 'precision': 0.25833333333333336, 'recall': 0.2108843537414966}}\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes classifier for each class\n",
    "nb_model = MultinomialNB()\n",
    "metrics_nb = {}\n",
    "\n",
    "for label in toxicity_types:\n",
    "    nb_model.fit(train_count, train_data[label])\n",
    "    preds = nb_model.predict(test_count)\n",
    "    metrics_nb[label] = {\n",
    "        'accuracy': accuracy_score(test_data[label], preds),\n",
    "        'f1': f1_score(test_data[label], preds),\n",
    "        'precision': precision_score(test_data[label], preds),\n",
    "        'recall': recall_score(test_data[label], preds)\n",
    "    }\n",
    "\n",
    "print(\"1st Approach (Count Vectorizer + Naive Bayes) Results:\", metrics_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxic</td>\n",
       "      <td>0.936300</td>\n",
       "      <td>0.689571</td>\n",
       "      <td>0.646436</td>\n",
       "      <td>0.738874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxic</td>\n",
       "      <td>0.985399</td>\n",
       "      <td>0.442584</td>\n",
       "      <td>0.359223</td>\n",
       "      <td>0.576324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>0.961053</td>\n",
       "      <td>0.665770</td>\n",
       "      <td>0.617764</td>\n",
       "      <td>0.721866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>0.996961</td>\n",
       "      <td>0.184874</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.148649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>0.958421</td>\n",
       "      <td>0.623760</td>\n",
       "      <td>0.575013</td>\n",
       "      <td>0.681537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.987153</td>\n",
       "      <td>0.232210</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.210884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Class  Accuracy  F1 Score  Precision    Recall\n",
       "0          toxic  0.936300  0.689571   0.646436  0.738874\n",
       "1   severe_toxic  0.985399  0.442584   0.359223  0.576324\n",
       "2        obscene  0.961053  0.665770   0.617764  0.721866\n",
       "3         threat  0.996961  0.184874   0.244444  0.148649\n",
       "4         insult  0.958421  0.623760   0.575013  0.681537\n",
       "5  identity_hate  0.987153  0.232210   0.258333  0.210884"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "# Collect data for each label from the metrics\n",
    "for label in toxicity_types:\n",
    "    data.append({\n",
    "        'Class': label,\n",
    "        'Accuracy': metrics_nb[label]['accuracy'],\n",
    "        'F1 Score': metrics_nb[label]['f1'],\n",
    "        'Precision': metrics_nb[label]['precision'],\n",
    "        'Recall': metrics_nb[label]['recall'],\n",
    "    })\n",
    "\n",
    "# Create the DataFrame\n",
    "metrics_df_nb = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + Logistic Regression Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IIT-J\\2nd year\\ML with big data\\Project\\youtube_comment_analysis\\yca-venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Vectorization using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 2), min_df=3, max_df=0.9)\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_data['comment_text'])\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Approach (TF-IDF + Logistic Regression) Results: {'toxic': {'accuracy': 0.9535014883283722, 'f1': 0.6956521739130435, 'precision': 0.9318681318681319, 'recall': 0.5549738219895288}, 'severe_toxic': {'accuracy': 0.9904433651887827, 'f1': 0.2823529411764706, 'precision': 0.5769230769230769, 'recall': 0.18691588785046728}, 'obscene': {'accuracy': 0.9734294219019269, 'f1': 0.6886930983847284, 'precision': 0.929633300297324, 'recall': 0.5469387755102041}, 'threat': {'accuracy': 0.9978693404355319, 'f1': 0.20930232558139536, 'precision': 0.75, 'recall': 0.12162162162162163}, 'insult': {'accuracy': 0.9692934356885478, 'f1': 0.608, 'precision': 0.8577878103837472, 'recall': 0.4708798017348203}, 'identity_hate': {'accuracy': 0.9916026946576845, 'f1': 0.21176470588235294, 'precision': 0.782608695652174, 'recall': 0.12244897959183673}}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression classifier for each class\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "metrics_lr = {}\n",
    "lr_models = {}\n",
    "\n",
    "# Use train_data to fit the model\n",
    "for label in toxicity_types:\n",
    "    lr_model.fit(train_tfidf, train_data[label])  # Corrected to use train_data\n",
    "    preds = lr_model.predict(test_tfidf)\n",
    "    metrics_lr[label] = {\n",
    "        'accuracy': accuracy_score(test_data[label], preds),\n",
    "        'f1': f1_score(test_data[label], preds),\n",
    "        'precision': precision_score(test_data[label], preds),\n",
    "        'recall': recall_score(test_data[label], preds)\n",
    "    }\n",
    "    lr_models[label] = lr_model\n",
    "print(\"2nd Approach (TF-IDF + Logistic Regression) Results:\", metrics_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxic</td>\n",
       "      <td>0.953501</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.931868</td>\n",
       "      <td>0.554974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxic</td>\n",
       "      <td>0.990443</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.186916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>0.973429</td>\n",
       "      <td>0.688693</td>\n",
       "      <td>0.929633</td>\n",
       "      <td>0.546939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>0.997869</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.121622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>0.969293</td>\n",
       "      <td>0.608000</td>\n",
       "      <td>0.857788</td>\n",
       "      <td>0.470880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.991603</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.122449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Class  Accuracy  F1 Score  Precision    Recall\n",
       "0          toxic  0.953501  0.695652   0.931868  0.554974\n",
       "1   severe_toxic  0.990443  0.282353   0.576923  0.186916\n",
       "2        obscene  0.973429  0.688693   0.929633  0.546939\n",
       "3         threat  0.997869  0.209302   0.750000  0.121622\n",
       "4         insult  0.969293  0.608000   0.857788  0.470880\n",
       "5  identity_hate  0.991603  0.211765   0.782609  0.122449"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "# Collect data for each label from the metrics\n",
    "for label in toxicity_types:\n",
    "    data.append({\n",
    "        'Class': label,\n",
    "        'Accuracy': metrics_lr[label]['accuracy'],\n",
    "        'F1 Score': metrics_lr[label]['f1'],\n",
    "        'Precision': metrics_lr[label]['precision'],\n",
    "        'Recall': metrics_lr[label]['recall'],\n",
    "    })\n",
    "\n",
    "# Create the DataFrame\n",
    "metrics_df_lr = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (GloVe) + LSTM Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization using Keras Tokenizer\n",
    "tokenizer = Tokenizer(num_words=20000)  # Limit vocabulary size\n",
    "tokenizer.fit_on_texts(train_data['comment_text'])\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['comment_text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding sequences to ensure same length\n",
    "max_seq_len = 100\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_seq_len)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings (GloVe embeddings are pre-downloaded)\n",
    "embedding_index = {}\n",
    "\n",
    "with open(\"../datasets/glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix\n",
    "embedding_dim = 100\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "def build_lstm_model():\n",
    "    model = Sequential([\n",
    "        Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=False),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.5),\n",
    "        Dense(6, activation='sigmoid')  # 6 output neurons for multi-label classification\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IIT-J\\2nd year\\ML with big data\\Project\\youtube_comment_analysis\\yca-venv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 119ms/step - accuracy: 0.5267 - loss: 0.1442 - val_accuracy: 0.9930 - val_loss: 0.0631\n",
      "Epoch 2/3\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 104ms/step - accuracy: 0.8777 - loss: 0.0634 - val_accuracy: 0.9939 - val_loss: 0.0586\n",
      "Epoch 3/3\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 107ms/step - accuracy: 0.9428 - loss: 0.0588 - val_accuracy: 0.9939 - val_loss: 0.0570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a02932f370>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and train the LSTM model\n",
    "lstm_model = build_lstm_model()\n",
    "lstm_model.fit(train_padded, train_data[toxicity_types].values, epochs=3, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m998/998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 22ms/step\n",
      "3rd Approach (Word Embeddings + LSTM) Results: {'toxic': {'accuracy': 0.95729280902397, 'f1': 0.7421002838221381, 'precision': 0.879766711529834, 'recall': 0.6416884816753927}, 'severe_toxic': {'accuracy': 0.9902553658154473, 'f1': 0.07164179104477612, 'precision': 0.8571428571428571, 'recall': 0.037383177570093455}, 'obscene': {'accuracy': 0.9765627447908507, 'f1': 0.7555555555555555, 'precision': 0.8594795539033457, 'recall': 0.6740524781341107}, 'threat': {'accuracy': 0.9976813410621964, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'insult': {'accuracy': 0.9691681027729907, 'f1': 0.6500711237553343, 'precision': 0.7629382303839732, 'recall': 0.5662949194547707}, 'identity_hate': {'accuracy': 0.990882030393232, 'f1': 0.026755852842809364, 'precision': 0.8, 'recall': 0.013605442176870748}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IIT-J\\2nd year\\ML with big data\\Project\\youtube_comment_analysis\\yca-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test data\n",
    "lstm_preds = lstm_model.predict(test_padded)\n",
    "metrics_lstm = {}\n",
    "\n",
    "for i, label in enumerate(toxicity_types):\n",
    "    lstm_bin_preds = (lstm_preds[:, i] > 0.5).astype(int)\n",
    "    metrics_lstm[label] = {\n",
    "        'accuracy': accuracy_score(test_data[label], lstm_bin_preds),\n",
    "        'f1': f1_score(test_data[label], lstm_bin_preds),\n",
    "        'precision': precision_score(test_data[label], lstm_bin_preds),\n",
    "        'recall': recall_score(test_data[label], lstm_bin_preds)\n",
    "    }\n",
    "\n",
    "print(\"3rd Approach (Word Embeddings + LSTM) Results:\", metrics_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxic</td>\n",
       "      <td>0.957293</td>\n",
       "      <td>0.742100</td>\n",
       "      <td>0.879767</td>\n",
       "      <td>0.641688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxic</td>\n",
       "      <td>0.990255</td>\n",
       "      <td>0.071642</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.037383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>0.976563</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.859480</td>\n",
       "      <td>0.674052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>0.997681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>0.969168</td>\n",
       "      <td>0.650071</td>\n",
       "      <td>0.762938</td>\n",
       "      <td>0.566295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>0.990882</td>\n",
       "      <td>0.026756</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.013605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Class  Accuracy  F1 Score  Precision    Recall\n",
       "0          toxic  0.957293  0.742100   0.879767  0.641688\n",
       "1   severe_toxic  0.990255  0.071642   0.857143  0.037383\n",
       "2        obscene  0.976563  0.755556   0.859480  0.674052\n",
       "3         threat  0.997681  0.000000   0.000000  0.000000\n",
       "4         insult  0.969168  0.650071   0.762938  0.566295\n",
       "5  identity_hate  0.990882  0.026756   0.800000  0.013605"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "# Collect data for each label from the metrics\n",
    "for label in toxicity_types:\n",
    "    data.append({\n",
    "        'Class': label,\n",
    "        'Accuracy': metrics_lstm[label]['accuracy'],\n",
    "        'F1 Score': metrics_lstm[label]['f1'],\n",
    "        'Precision': metrics_lstm[label]['precision'],\n",
    "        'Recall': metrics_lstm[label]['recall'],\n",
    "    })\n",
    "\n",
    "# Create the DataFrame\n",
    "metrics_df_lstm = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparing all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting average scores from each approach for all metrics\n",
    "approach_metrics = {\n",
    "    'TF-IDF + Logistic Regression': metrics_lr,\n",
    "    'Count Vectorizer + Naive Bayes': metrics_nb,\n",
    "    'Word Embeddings + LSTM': metrics_lstm\n",
    "}\n",
    "\n",
    "# Averaging the metrics across all six labels\n",
    "def average_metrics(metrics_dict):\n",
    "    avg_metrics = {'accuracy': [], 'f1': [], 'precision': [], 'recall': []}\n",
    "    for metric in avg_metrics:\n",
    "        avg_metrics[metric] = np.mean([metrics_dict[label][metric] for label in toxicity_types])\n",
    "    return avg_metrics\n",
    "\n",
    "# Calculate average metrics for each approach\n",
    "avg_metrics_lr = average_metrics(metrics_lr)\n",
    "avg_metrics_nb = average_metrics(metrics_nb)\n",
    "avg_metrics_lstm = average_metrics(metrics_lstm)\n",
    "\n",
    "# Data for bar plot\n",
    "labels = ['accuracy', 'f1', 'precision', 'recall']\n",
    "lr_scores = [avg_metrics_lr[m] for m in labels]\n",
    "nb_scores = [avg_metrics_nb[m] for m in labels]\n",
    "lstm_scores = [avg_metrics_lstm[m] for m in labels]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(x - width, lr_scores, width, label='TF-IDF + Logistic Regression')\n",
    "ax.bar(x, nb_scores, width, label='Count Vectorizer + Naive Bayes')\n",
    "ax.bar(x + width, lstm_scores, width, label='Word Embeddings + LSTM')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Comparison of Classification Approaches')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Count Vectorizer + Naive Bayes\n",
    "\n",
    "    Accuracy: Decent (~0.93 to 0.98)\n",
    "\n",
    "    F1 Scores: Moderate (e.g., 0.68 for toxic, 0.44 for severe_toxic, very low for threat)\n",
    "\n",
    "    Recall: Stronger than TF-IDF in some classes (like insult)\n",
    "\n",
    "    But fails badly on threat (F1 = 0.18) and identity_hate (F1 = 0.23)\n",
    "\n",
    "2. TF-IDF + Logistic Regression\n",
    "\n",
    "    Highest accuracy across most classes (esp. threat, insult, identity_hate)\n",
    "\n",
    "    High precision (esp. toxic, obscene, threat, insult)\n",
    "\n",
    "    Weak recall on multiple classes (threat, severe_toxic, identity_hate)\n",
    "\n",
    "    Lower F1 score on some classes due to imbalance between precision & recall\n",
    "\n",
    "3. Word Embeddings + LSTM\n",
    "\n",
    "    Best F1 scores overall (especially toxic, obscene, insult)\n",
    "\n",
    "    Balanced precision and recall for key labels\n",
    "\n",
    "    However, completely fails on threat and identity_hate (F1 = 0), indicating class imbalance issues or model underfitting those classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Collect data for each class from all models\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_label \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlabel_cols\u001b[49m:\n\u001b[0;32m      6\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m: class_label,\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF-IDF + Logistic Regression Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics_lr[class_label][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord Embeddings + LSTM Recall\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics_lstm[class_label][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     20\u001b[0m     })\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Create the DataFrame\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_cols' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a list to hold the data for the DataFrame\n",
    "data = []\n",
    "\n",
    "# Collect data for each class from all models\n",
    "for class_label in label_cols:\n",
    "    data.append({\n",
    "        'Class': class_label,\n",
    "        'TF-IDF + Logistic Regression Accuracy': metrics_lr[class_label]['accuracy'],\n",
    "        'TF-IDF + Logistic Regression F1 Score': metrics_lr[class_label]['f1'],\n",
    "        'TF-IDF + Logistic Regression Precision': metrics_lr[class_label]['precision'],\n",
    "        'TF-IDF + Logistic Regression Recall': metrics_lr[class_label]['recall'],\n",
    "        'Count Vectorizer + Naive Bayes Accuracy': metrics_nb[class_label]['accuracy'],\n",
    "        'Count Vectorizer + Naive Bayes F1 Score': metrics_nb[class_label]['f1'],\n",
    "        'Count Vectorizer + Naive Bayes Precision': metrics_nb[class_label]['precision'],\n",
    "        'Count Vectorizer + Naive Bayes Recall': metrics_nb[class_label]['recall'],\n",
    "        'Word Embeddings + LSTM Accuracy': metrics_lstm[class_label]['accuracy'],\n",
    "        'Word Embeddings + LSTM F1 Score': metrics_lstm[class_label]['f1'],\n",
    "        'Word Embeddings + LSTM Precision': metrics_lstm[class_label]['precision'],\n",
    "        'Word Embeddings + LSTM Recall': metrics_lstm[class_label]['recall'],\n",
    "    })\n",
    "\n",
    "# Create the DataFrame\n",
    "metrics_df = pd.DataFrame(data)\n",
    "\n",
    "metrics_df_rotated = metrics_df.T\n",
    "\n",
    "# Display the rotated DataFrame\n",
    "metrics_df_rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "By looking at the above summary we will use the TF-IDF + Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TF-IDF vectorizer\n",
    "with open('../models/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "# Save the Logistic Regression models\n",
    "with open('../models/lr_models.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying models...\n",
      "\n",
      "Predictions for sample text:\n",
      "toxic: 0\n",
      "severe_toxic: 0\n",
      "obscene: 0\n",
      "threat: 0\n",
      "insult: 0\n",
      "identity_hate: 0\n"
     ]
    }
   ],
   "source": [
    "# 5. Verification step\n",
    "print(\"\\nVerifying models...\")\n",
    "# Test prediction on a sample\n",
    "sample_text = test_data['comment_text'].iloc[0]\n",
    "sample_tfidf = tfidf_vectorizer.transform([sample_text])\n",
    "print(\"\\nPredictions for sample text:\")\n",
    "for label in toxicity_types:\n",
    "    pred = lr_models[label].predict(sample_tfidf)[0]\n",
    "    print(f\"{label}: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yca-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
