{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Q\\AppData\\Local\\Temp\\ipykernel_39688\\2529523849.py:156: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../datasets/processed/final_train_data.csv\")\n",
      "Training Epoch 1: 100%|██████████| 9856/9856 [1:03:18<00:00,  2.59it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.4164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9856/9856 [1:51:42<00:00,  1.47it/s]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.2631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9856/9856 [48:06<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.1648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 9856/9856 [1:05:45<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Loss: 0.1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 9856/9856 [1:09:39<00:00,  2.36it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Loss: 0.0656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1232/1232 [03:09<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spam Classification Report\n",
      "Accuracy: 0.9989\n",
      "F1 Score: 0.8750\n",
      "\n",
      "Sentiment Classification Report\n",
      "Accuracy: 0.9642\n",
      "F1 Macro: 0.9093\n",
      "\n",
      "Toxicity Classification Report (Multi-Label)\n",
      "F1 Macro: 0.5466\n",
      "\n",
      "Hate Speech Classification Report\n",
      "Accuracy: 1.0000\n",
      "F1 Macro: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define dataset\n",
    "class YouTubeCommentDataset(Dataset):\n",
    "    def __init__(self, texts, spam_labels, sentiment_labels, toxicity_labels, hate_speech_labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.spam_labels = spam_labels\n",
    "        self.sentiment_labels = sentiment_labels\n",
    "        self.toxicity_labels = toxicity_labels\n",
    "        self.hate_speech_labels = hate_speech_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'spam_label': torch.tensor(self.spam_labels[idx], dtype=torch.float),\n",
    "            'sentiment_label': torch.tensor(self.sentiment_labels[idx], dtype=torch.long),\n",
    "            'toxicity_labels': torch.tensor(self.toxicity_labels[idx], dtype=torch.float),\n",
    "            'hate_speech_label': torch.tensor(self.hate_speech_labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Define model\n",
    "class MultiTaskXLMRoberta(nn.Module):\n",
    "    def __init__(self, model_name, num_sentiment_classes, num_toxicity_labels, num_hate_speech_classes):\n",
    "        super(MultiTaskXLMRoberta, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.spam_head = nn.Linear(hidden_size, 1)\n",
    "        self.sentiment_head = nn.Linear(hidden_size, num_sentiment_classes)\n",
    "        self.toxicity_head = nn.Linear(hidden_size, num_toxicity_labels)\n",
    "        self.hate_speech_head = nn.Linear(hidden_size, num_hate_speech_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0]  # XLM-Roberta CLS token\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        \n",
    "        return {\n",
    "            'spam': self.spam_head(cls_output),\n",
    "            'sentiment': self.sentiment_head(cls_output),\n",
    "            'toxicity': self.toxicity_head(cls_output),\n",
    "            'hate_speech': self.hate_speech_head(cls_output)\n",
    "        }\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, val_loader, device, epochs=5):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, 0, total_steps)\n",
    "\n",
    "    spam_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    sentiment_loss_fn = nn.CrossEntropyLoss()\n",
    "    toxicity_loss_fn = nn.BCEWithLogitsLoss()\n",
    "    hate_loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            spam_label = batch['spam_label'].to(device)\n",
    "            sentiment_label = batch['sentiment_label'].to(device)\n",
    "            toxicity_labels = batch['toxicity_labels'].to(device)\n",
    "            hate_label = batch['hate_speech_label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = (\n",
    "                spam_loss_fn(outputs['spam'].squeeze(), spam_label) +\n",
    "                sentiment_loss_fn(outputs['sentiment'], sentiment_label) +\n",
    "                toxicity_loss_fn(outputs['toxicity'], toxicity_labels) +\n",
    "                hate_loss_fn(outputs['hate_speech'], hate_label)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_spam_preds, all_spam_labels = [], []\n",
    "    all_sentiment_preds, all_sentiment_labels = [], []\n",
    "    all_toxicity_preds, all_toxicity_labels = [], []\n",
    "    all_hate_preds, all_hate_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            spam_label = batch['spam_label'].to(device)\n",
    "            sentiment_label = batch['sentiment_label'].to(device)\n",
    "            toxicity_labels = batch['toxicity_labels'].to(device)\n",
    "            hate_label = batch['hate_speech_label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            all_spam_preds.extend(torch.sigmoid(outputs['spam']).squeeze().cpu().numpy() > 0.5)\n",
    "            all_spam_labels.extend(spam_label.cpu().numpy())\n",
    "\n",
    "            all_sentiment_preds.extend(torch.argmax(outputs['sentiment'], dim=1).cpu().numpy())\n",
    "            all_sentiment_labels.extend(sentiment_label.cpu().numpy())\n",
    "\n",
    "            all_toxicity_preds.extend((torch.sigmoid(outputs['toxicity']).cpu().numpy() > 0.5).astype(int))\n",
    "            all_toxicity_labels.extend(toxicity_labels.cpu().numpy())\n",
    "\n",
    "            all_hate_preds.extend(torch.argmax(outputs['hate_speech'], dim=1).cpu().numpy())\n",
    "            all_hate_labels.extend(hate_label.cpu().numpy())\n",
    "\n",
    "    # Metrics\n",
    "    print(\"\\nSpam Classification Report\")\n",
    "    print(f\"Accuracy: {accuracy_score(all_spam_labels, all_spam_preds):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(all_spam_labels, all_spam_preds):.4f}\")\n",
    "\n",
    "    print(\"\\nSentiment Classification Report\")\n",
    "    print(f\"Accuracy: {accuracy_score(all_sentiment_labels, all_sentiment_preds):.4f}\")\n",
    "    print(f\"F1 Macro: {f1_score(all_sentiment_labels, all_sentiment_preds, average='macro'):.4f}\")\n",
    "\n",
    "    print(\"\\nToxicity Classification Report (Multi-Label)\")\n",
    "    print(f\"F1 Macro: {f1_score(np.array(all_toxicity_labels), np.array(all_toxicity_preds), average='macro'):.4f}\")\n",
    "\n",
    "    print(\"\\nHate Speech Classification Report\")\n",
    "    print(f\"Accuracy: {accuracy_score(all_hate_labels, all_hate_preds):.4f}\")\n",
    "    print(f\"F1 Macro: {f1_score(all_hate_labels, all_hate_preds, average='macro'):.4f}\")\n",
    "\n",
    "# Main\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load and clean data\n",
    "    df = pd.read_csv(\"../datasets/processed/final_train_data.csv\")\n",
    "\n",
    "    # Tasks info\n",
    "    tasks = {\n",
    "        'spam': 'spam',\n",
    "        'sentiment': 'sentiment',\n",
    "        'toxicity': ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
    "        'hate_speech': 'hate_speech'\n",
    "    }\n",
    "\n",
    "    # Clean columns\n",
    "    df['spam'] = pd.to_numeric(df['spam'], errors='coerce').fillna(0).astype(int)\n",
    "    df['hate_speech'] = pd.to_numeric(df['hate_speech'], errors='coerce').fillna(0).astype(int)\n",
    "    for col in tasks['toxicity']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # Map sentiment text to indices\n",
    "    sentiment_map = {'Positive': 0, 'Neutral': 1, 'Negative': 2, 'Irrelevant': 3}\n",
    "    df['sentiment'] = df['sentiment'].map(sentiment_map).fillna(1).astype(int)\n",
    "\n",
    "    # Prepare data\n",
    "    X = df['text'].fillna(\"\").tolist()\n",
    "    spam_y = df['spam'].tolist()\n",
    "    sentiment_y = df['sentiment'].tolist()\n",
    "    toxicity_y = df[tasks['toxicity']].values.tolist()\n",
    "    hate_y = df['hate_speech'].tolist()\n",
    "\n",
    "    # Split\n",
    "    train_idx, val_idx = train_test_split(range(len(X)), test_size=0.2, random_state=42)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "    train_dataset = YouTubeCommentDataset(\n",
    "        [X[i] for i in train_idx],\n",
    "        [spam_y[i] for i in train_idx],\n",
    "        [sentiment_y[i] for i in train_idx],\n",
    "        [toxicity_y[i] for i in train_idx],\n",
    "        [hate_y[i] for i in train_idx],\n",
    "        tokenizer\n",
    "    )\n",
    "\n",
    "    val_dataset = YouTubeCommentDataset(\n",
    "        [X[i] for i in val_idx],\n",
    "        [spam_y[i] for i in val_idx],\n",
    "        [sentiment_y[i] for i in val_idx],\n",
    "        [toxicity_y[i] for i in val_idx],\n",
    "        [hate_y[i] for i in val_idx],\n",
    "        tokenizer\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    # Model\n",
    "    model = MultiTaskXLMRoberta(\n",
    "        model_name=\"xlm-roberta-base\",\n",
    "        num_sentiment_classes=4,\n",
    "        num_toxicity_labels=6,\n",
    "        num_hate_speech_classes=3\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    train(model, train_loader, val_loader, device, epochs=5)\n",
    "\n",
    "    # Evaluate\n",
    "    evaluate(model, val_loader, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yca-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
