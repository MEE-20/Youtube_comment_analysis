{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge Processed Comment Datasets into Unified Multi-Task Corpus\n",
    "##### This notebook loads your four processed CSVs, aligns their schemas,\n",
    "##### applies rule-based label logic, and saves a single merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Now you can import from src\n",
    "from src.utils.utils import clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaw_df      = pd.read_csv('../datasets/processed/toxic_comments_relevant_data.csv')\n",
    "hatexplain_df  = pd.read_csv('../datasets/processed/hatexplain_relevant_data.csv')\n",
    "spam_df        = pd.read_csv('../datasets/processed/youtube_spam_relevant_data.csv')\n",
    "sentiment_df   = pd.read_csv('../datasets/processed/twitter_relevant_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jigsaw: (159571, 8)\n",
      "HateXplain: (15383, 2)\n",
      "Spam: (1956, 2)\n",
      "Sentiment: (69491, 2)\n"
     ]
    }
   ],
   "source": [
    "# Preview shapes:\n",
    "print(\"Jigsaw:\", jigsaw_df.shape)\n",
    "print(\"HateXplain:\", hatexplain_df.shape)\n",
    "print(\"Spam:\", spam_df.shape)\n",
    "print(\"Sentiment:\", sentiment_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  none  \n",
       "0        0       0       0              0     1  \n",
       "1        0       0       0              0     1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jigsaw_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>majority_label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>offensive</td>\n",
       "      <td>u really think i would not have been raped by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>offensive</td>\n",
       "      <td>the uk has threatened to return radioactive wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  majority_label                                            message\n",
       "0      offensive  u really think i would not have been raped by ...\n",
       "1      offensive  the uk has threatened to return radioactive wa..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hatexplain_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             CONTENT  CLASS\n",
       "0  Huh, anyway check out this you[tube] channel: ...      1\n",
       "1  Hey guys check out my new channel and our firs...      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  im getting on borderlands and i will murder yo...  Positive\n",
       "1  I am coming to the borders and I will kill you...  Positive"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Standardize Schemas\n",
    " We want each DF to have: `text`, six toxicity sub-labels,\n",
    " `hate_speech`, `spam`, `sentiment`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### >>>>>> rename text/message columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaw_df   = jigsaw_df.rename(columns={'comment_text':'text'}).drop(columns=['none'])\n",
    "hatexplain_df = hatexplain_df.rename(columns={'message':'text'})\n",
    "spam_df     = spam_df.rename(columns={'CONTENT':'text'})\n",
    "# sentiment_df already has `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tox_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "# Jigsaw: already has tox_cols; add missing task columns\n",
    "jigsaw_df['hate_speech'] = np.nan\n",
    "jigsaw_df['spam']        = np.nan\n",
    "jigsaw_df['sentiment']   = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HateXplain: add tox_cols, spam, sentiment\n",
    "hatexplain_df = hatexplain_df.rename(columns={'majority_label': 'hate_speech'})\n",
    "for col in tox_cols:\n",
    "    hatexplain_df[col] = np.nan\n",
    "hatexplain_df['spam']      = np.nan\n",
    "hatexplain_df['sentiment'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam: rename column, add tox_cols, hate_speech, sentiment\n",
    "spam_df = spam_df.rename(columns={'CLASS': 'spam'})\n",
    "for col in tox_cols:\n",
    "    spam_df[col] = np.nan\n",
    "spam_df['hate_speech'] = np.nan\n",
    "# Map spam labels to strings\n",
    "spam_df['spam'] = spam_df['spam'].map({1: 'spam', 0: 'ham'})\n",
    "spam_df['sentiment'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment140: add tox_cols, hate_speech, spam\n",
    "for col in tox_cols:\n",
    "    sentiment_df[col] = np.nan\n",
    "sentiment_df['hate_speech'] = np.nan\n",
    "sentiment_df['spam']        = np.nan\n",
    "\n",
    "# Map sentiment labels to strings\n",
    "sentiment_df['sentiment'] = sentiment_df['sentiment'].map({0: 'negative', 2: 'neutral', 4: 'positive'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Re-)Clean Text\n",
    "Ensure consistent text preprocessing across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "for df in [jigsaw_df, hatexplain_df, spam_df, sentiment_df]:\n",
    "    df['text'] = df['text'].astype(str).apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Combine All DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined shape: (246401, 10)\n"
     ]
    }
   ],
   "source": [
    "combined = pd.concat([\n",
    "    jigsaw_df[['text'] + tox_cols + ['hate_speech', 'spam', 'sentiment']],\n",
    "    hatexplain_df[['text'] + tox_cols + ['hate_speech', 'spam', 'sentiment']],\n",
    "    spam_df[['text'] + tox_cols + ['hate_speech', 'spam', 'sentiment']],\n",
    "    sentiment_df[['text'] + tox_cols + ['hate_speech', 'spam', 'sentiment']]\n",
    "], ignore_index=True)\n",
    "print(\"Combined shape:\", combined.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Apply Rule-Based Logic for Toxicity for hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'offensive', 'normal', 'hatespeech'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined['hate_speech'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Q\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Q\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Define the tokenize function\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Load the models\n",
    "models_path = project_root / 'models'\n",
    "\n",
    "# Load TF-IDF vectorizer\n",
    "with open(models_path / 'tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "# Load Logistic Regression model\n",
    "with open(models_path / 'lr_models.pkl', 'rb') as f:\n",
    "    lr_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1: Predict toxicity sub-labels for 'hate' or 'offensive' comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_offender = combined['hate_speech'].isin(['hate','offensive'])\n",
    "# if mask_offender.any():\n",
    "#     X_off = tfidf_vectorizer.transform(combined.loc[mask_offender,'text'])\n",
    "    \n",
    "#     # Get predictions for all toxicity types\n",
    "#     predictions = lr_model.predict(X_off)\n",
    "    \n",
    "#     # Update all toxicity columns at once\n",
    "#     combined.loc[mask_offender, tox_cols] = predictions\n",
    "    \n",
    "#     # If you want to keep only the highest confidence prediction\n",
    "#     def top_label(vals):\n",
    "#         arr = np.array(vals, dtype=float)\n",
    "#         max_val = arr.max()\n",
    "#         return [1 if a==max_val and max_val>0 else 0 for a in arr]\n",
    "    \n",
    "#     top_df = combined.loc[mask_offender, tox_cols].apply(top_label, axis=1, result_type='expand')\n",
    "#     top_df.columns = tox_cols\n",
    "#     combined.loc[mask_offender, tox_cols] = top_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4384 comments flagged as hate speech or offensive\n",
      "Predicting toxic...\n",
      "Predicting severe_toxic...\n",
      "Predicting obscene...\n",
      "Predicting threat...\n",
      "Predicting insult...\n",
      "Predicting identity_hate...\n",
      "\n",
      "Refining predictions to keep only the strongest toxicity type...\n"
     ]
    }
   ],
   "source": [
    "# Get comments flagged as hate speech or offensive\n",
    "mask_offender = combined['hate_speech'].isin(['hate', 'offensive'])\n",
    "print(f\"Found {mask_offender.sum()} comments flagged as hate speech or offensive\")\n",
    "\n",
    "if mask_offender.any():\n",
    "    # Transform the text data\n",
    "    X_off = tfidf_vectorizer.transform(combined.loc[mask_offender, 'text'])\n",
    "    \n",
    "    # Predict each toxicity type\n",
    "    for label in tox_cols:\n",
    "        print(f\"Predicting {label}...\")\n",
    "        predictions = lr_models[label].predict(X_off)\n",
    "        combined.loc[mask_offender, label] = predictions\n",
    "    \n",
    "    # Optional: Keep only the highest confidence prediction for each comment\n",
    "    print(\"\\nRefining predictions to keep only the strongest toxicity type...\")\n",
    "    def top_label(vals):\n",
    "        arr = np.array(vals, dtype=float)\n",
    "        max_val = arr.max()\n",
    "        return [1 if a == max_val and max_val > 0 else 0 for a in arr]\n",
    "    \n",
    "    top_df = combined.loc[mask_offender, tox_cols].apply(top_label, axis=1, result_type='expand')\n",
    "    top_df.columns = tox_cols\n",
    "    combined.loc[mask_offender, tox_cols] = top_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Summary:\n",
      "toxic: 15358.0 comments (6.23%)\n",
      "severe_toxic: 1659.0 comments (0.67%)\n",
      "obscene: 8513.0 comments (3.45%)\n",
      "threat: 542.0 comments (0.22%)\n",
      "insult: 7941.0 comments (3.22%)\n",
      "identity_hate: 1469.0 comments (0.60%)\n"
     ]
    }
   ],
   "source": [
    "# Print summary of predictions\n",
    "print(\"\\nPrediction Summary:\")\n",
    "for col in tox_cols:\n",
    "    count = combined[col].sum()\n",
    "    percent = (count / len(combined)) * 100\n",
    "    print(f\"{col}: {count} comments ({percent:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of toxic comments with their classifications:\n",
      "\n",
      "Text: gibraltarian get lost. do not contact me again under any pretext. you behaviour is beneath contempt. ...\n",
      "Hate Speech Label: nan\n",
      "Toxicity Types: ['toxic']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Text: hello dickbrain just another wikipedia wanker ...\n",
      "Hate Speech Label: nan\n",
      "Toxicity Types: ['toxic', 'obscene', 'insult']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Text: because i did not do anything wrong in the first place..you accused me of doing something i did not  ...\n",
      "Hate Speech Label: nan\n",
      "Toxicity Types: ['toxic']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Text: if you see this message wikipedia's talk system is terrible, feel free to direct me to the appropria ...\n",
      "Hate Speech Label: nan\n",
      "Toxicity Types: ['toxic', 'obscene', 'insult']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Text: latin americans have to be some of the most ignorant people in the world. get a real education, and  ...\n",
      "Hate Speech Label: nan\n",
      "Toxicity Types: ['toxic', 'insult', 'identity_hate']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display a sample of toxic comments\n",
    "print(\"\\nSample of toxic comments with their classifications:\")\n",
    "toxic_mask = combined[tox_cols].any(axis=1)\n",
    "sample_size = min(5, toxic_mask.sum())\n",
    "sample_df = combined[toxic_mask].sample(n=sample_size)\n",
    "for _, row in sample_df.iterrows():\n",
    "    print(\"\\nText:\", row['text'][:100], \"...\")  # Show first 100 chars\n",
    "    print(\"Hate Speech Label:\", row['hate_speech'])\n",
    "    print(\"Toxicity Types:\", [col for col in tox_cols if row[col] == 1])\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20609 comments with toxic/offensive content\n",
      "\n",
      "Before update:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "After update:\n",
      "sentiment\n",
      "negative    20609\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample of updated sentiments:\n",
      "\n",
      "Total sentiments changed: 246401\n",
      "\n",
      "Text: why the hell...? why the hell do you erase data from nationwide anthropometric studies e.g. iran, cz ...\n",
      "Original Sentiment: nan\n",
      "New Sentiment: negative\n",
      "Reason: Toxic labels: ['toxic']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Text: yea home depot was not like this while i worked there ...\n",
      "Original Sentiment: nan\n",
      "New Sentiment: nan\n",
      "Reason: --------------------------------------------------------------------------------\n",
      "\n",
      "Text: you are so right. i am just going to go back to my regular of working, which is to almost never comm ...\n",
      "Original Sentiment: nan\n",
      "New Sentiment: nan\n",
      "Reason: --------------------------------------------------------------------------------\n",
      "\n",
      "Text: as in sony buying another console generation over xbox? yes. the playstation brand is the best sold  ...\n",
      "Original Sentiment: nan\n",
      "New Sentiment: nan\n",
      "Reason: --------------------------------------------------------------------------------\n",
      "\n",
      "Text: ubisoft have plans to fix the slide in gr breakpoint. the guy goes on a full slide down the driveway ...\n",
      "Original Sentiment: nan\n",
      "New Sentiment: nan\n",
      "Reason: --------------------------------------------------------------------------------\n",
      "\n",
      "Saved processed dataset to: D:\\IIT-J\\2nd year\\ML with big data\\Project\\youtube_comment_analysis\\datasets\\processed\\final_comment_analysis_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a mask for any type of toxic or offensive content\n",
    "toxic_mask = (\n",
    "    # Check for any toxicity label\n",
    "    combined[tox_cols].any(axis=1) |\n",
    "    # Check for hate speech or offensive content\n",
    "    combined['hate_speech'].isin(['hate', 'offensive'])\n",
    ")\n",
    "\n",
    "# Update sentiment to negative where toxic_mask is True\n",
    "print(f\"Found {toxic_mask.sum()} comments with toxic/offensive content\")\n",
    "print(\"\\nBefore update:\")\n",
    "print(combined['sentiment'].value_counts())\n",
    "\n",
    "# Create backup of original sentiment if needed\n",
    "combined['original_sentiment'] = combined['sentiment']\n",
    "\n",
    "# Update sentiment to 'negative' where toxic_mask is True\n",
    "combined.loc[toxic_mask, 'sentiment'] = 'negative'\n",
    "\n",
    "print(\"\\nAfter update:\")\n",
    "print(combined['sentiment'].value_counts())\n",
    "\n",
    "# Show some examples of updated sentiments\n",
    "print(\"\\nSample of updated sentiments:\")\n",
    "changed_mask = (combined['sentiment'] != combined['original_sentiment'])\n",
    "print(f\"\\nTotal sentiments changed: {changed_mask.sum()}\")\n",
    "\n",
    "sample_size = min(5, changed_mask.sum())\n",
    "if sample_size > 0:\n",
    "    sample_df = combined[changed_mask].sample(n=sample_size)\n",
    "    for _, row in sample_df.iterrows():\n",
    "        print(\"\\nText:\", row['text'][:100], \"...\")  # Show first 100 chars\n",
    "        print(\"Original Sentiment:\", row['original_sentiment'])\n",
    "        print(\"New Sentiment:\", row['sentiment'])\n",
    "        print(\"Reason:\", end=\" \")\n",
    "        if row[tox_cols].any():\n",
    "            print(\"Toxic labels:\", [col for col in tox_cols if row[col] == 1])\n",
    "        if row['hate_speech'] in ['hate', 'offensive']:\n",
    "            print(\"Hate speech label:\", row['hate_speech'])\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "output_path = project_root / 'datasets' / 'processed' / 'final_comment_analysis_data.csv'\n",
    "combined.to_csv(output_path, index=False)\n",
    "print(f\"\\nSaved processed dataset to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yca-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
