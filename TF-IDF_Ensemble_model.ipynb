{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import scipy.sparse as sp\n",
    "import warnings\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import FastText\n",
    "from gensim.utils import simple_preprocess\n",
    "import multiprocessing\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define task structure\n",
    "tasks = {\n",
    "    'spam': {'type': 'binary', 'column': 'spam'},\n",
    "    'sentiment': {'type': 'multi-class', 'column': 'sentiment', 'classes': ['Positive', 'Neutral', 'Negative', 'Irrelevant']},\n",
    "    'toxicity': {'type': 'multi-label', 'columns': ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']},\n",
    "    'hate_speech': {'type': 'multi-class', 'column': 'hate_speech', 'classes': ['normal', 'offensive', 'hatespeech']}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text for FastText\n",
    "def preprocess_for_fasttext(text):\n",
    "    return simple_preprocess(text, deacc=True)  # deacc=True removes punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n"
     ]
    }
   ],
   "source": [
    "# Load the provided CSV files\n",
    "print(\"Loading data files...\")\n",
    "df_train = pd.read_csv('../datasets/processed/final_train_data.csv')\n",
    "df_test = pd.read_csv('../datasets/processed/final_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (197118, 10)\n",
      "Testing data shape: (49283, 10)\n"
     ]
    }
   ],
   "source": [
    "# Print data info\n",
    "print(f\"Training data shape: {df_train.shape}\")\n",
    "print(f\"Testing data shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text columns\n",
    "X_train = df_train['text']\n",
    "X_test = df_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['text'].fillna('').astype(str)\n",
    "X_test = df_test['text'].fillna('').astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text for FastText...\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text for FastText\n",
    "print(\"Preprocessing text for FastText...\")\n",
    "df_train['processed_text'] = df_train['text'].fillna('').astype(str).apply(preprocess_for_fasttext)\n",
    "df_test['processed_text'] = df_test['text'].fillna('').astype(str).apply(preprocess_for_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing FastText model from ../src/models/fasttext_model.model\n"
     ]
    }
   ],
   "source": [
    "# Create or load FastText model\n",
    "fasttext_model_path = '../src/models/fasttext_model.model'\n",
    "if os.path.exists(fasttext_model_path):\n",
    "    print(f\"Loading existing FastText model from {fasttext_model_path}\")\n",
    "    fasttext_model = FastText.load(fasttext_model_path)\n",
    "else:\n",
    "    print(\"Training FastText model...\")\n",
    "    # Use all available cores for faster training\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    \n",
    "    # Train FastText model\n",
    "    fasttext_model = FastText(\n",
    "        vector_size=100,     # Embedding dimension\n",
    "        window=5,            # Context window size\n",
    "        min_count=5,         # Minimum word count\n",
    "        workers=cores,       # Use all available cores\n",
    "        sg=1                 # Use skip-gram model (1) instead of CBOW (0)\n",
    "    )\n",
    "    \n",
    "    # Build vocabulary\n",
    "    fasttext_model.build_vocab(df_train['processed_text'].tolist())\n",
    "    \n",
    "    # Train the model\n",
    "    fasttext_model.train(\n",
    "        df_train['processed_text'].tolist(),\n",
    "        total_examples=len(df_train),\n",
    "        epochs=10\n",
    "    )\n",
    "    \n",
    "    # Save the model\n",
    "    fasttext_model.save(fasttext_model_path)\n",
    "    print(f\"FastText model saved to {fasttext_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get document vector from FastText\n",
    "def get_document_vector(text, model, vector_size=100):\n",
    "    words = preprocess_for_fasttext(text)\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    \n",
    "    if len(word_vectors) > 0:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FastText document vectors...\n"
     ]
    }
   ],
   "source": [
    "# Create document vectors for training and test sets\n",
    "print(\"Creating FastText document vectors...\")\n",
    "train_fasttext_vectors = np.array([get_document_vector(text, fasttext_model) for text in X_train])\n",
    "test_fasttext_vectors = np.array([get_document_vector(text, fasttext_model) for text in X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF features\n",
    "print(\"Creating TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features shape: (197118, 10000)\n",
      "FastText features shape: (197118, 100)\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF features shape: {X_train_tfidf.shape}\")\n",
    "print(f\"FastText features shape: {train_fasttext_vectors.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining TF-IDF and FastText features...\n",
      "Combined features shape: (197118, 10100)\n"
     ]
    }
   ],
   "source": [
    "# Combine TF-IDF and FastText features\n",
    "print(\"Combining TF-IDF and FastText features...\")\n",
    "X_train_combined = sp.hstack((X_train_tfidf, sp.csr_matrix(train_fasttext_vectors)))\n",
    "X_test_combined = sp.hstack((X_test_tfidf, sp.csr_matrix(test_fasttext_vectors)))\n",
    "\n",
    "print(f\"Combined features shape: {X_train_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate models for each task\n",
    "def train_and_evaluate_models():\n",
    "    models = {}\n",
    "    \n",
    "    # 1. Binary Classification: Spam\n",
    "    spam_col = tasks['spam']['column']\n",
    "    # Filter out rows with missing labels\n",
    "    spam_train_mask = ~df_train[spam_col].isna()\n",
    "    if spam_train_mask.any():\n",
    "        y_spam_train = df_train.loc[spam_train_mask, spam_col].astype(int)\n",
    "        X_spam_train = X_train_combined[spam_train_mask]\n",
    "        \n",
    "        print(f\"Training spam classifier with {len(y_spam_train)} samples\")\n",
    "        \n",
    "        spam_model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "        spam_model.fit(X_spam_train, y_spam_train)\n",
    "        models['spam'] = spam_model\n",
    "    else:\n",
    "        print(\"No spam labels found in training data. Skipping spam classifier.\")\n",
    "    \n",
    "    # 2. Multi-class Classification: Sentiment\n",
    "    sentiment_col = tasks['sentiment']['column']\n",
    "    sentiment_train_mask = ~df_train[sentiment_col].isna()\n",
    "    if sentiment_train_mask.any():\n",
    "        y_sentiment_train = df_train.loc[sentiment_train_mask, sentiment_col]\n",
    "        X_sentiment_train = X_train_combined[sentiment_train_mask]\n",
    "        \n",
    "        # Label encode sentiment classes\n",
    "        sentiment_encoder = LabelEncoder()\n",
    "        y_sentiment_train_encoded = sentiment_encoder.fit_transform(y_sentiment_train)\n",
    "        \n",
    "        print(f\"Training sentiment classifier with {len(y_sentiment_train)} samples\")\n",
    "        \n",
    "        sentiment_model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            objective='multi:softprob',\n",
    "            num_class=len(np.unique(y_sentiment_train_encoded)),\n",
    "            eval_metric='mlogloss',\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "        sentiment_model.fit(X_sentiment_train, y_sentiment_train_encoded)\n",
    "        models['sentiment'] = {\n",
    "            'model': sentiment_model,\n",
    "            'encoder': sentiment_encoder,\n",
    "            'classes': sorted(list(sentiment_encoder.classes_))\n",
    "        }\n",
    "    else:\n",
    "        print(\"No sentiment labels found in training data. Skipping sentiment classifier.\")\n",
    "    \n",
    "    # 3. Multi-label Classification: Toxicity\n",
    "    toxicity_cols = tasks['toxicity']['columns']\n",
    "    # For multi-label, we handle each column as a separate binary classification\n",
    "    toxicity_models = {}\n",
    "    \n",
    "    for col in toxicity_cols:\n",
    "        # Filter rows with available labels\n",
    "        col_train_mask = ~df_train[col].isna()\n",
    "        if col_train_mask.any():\n",
    "            y_col_train = df_train.loc[col_train_mask, col].astype(int)\n",
    "            X_col_train = X_train_combined[col_train_mask]\n",
    "            \n",
    "            print(f\"Training {col} classifier with {len(y_col_train)} samples\")\n",
    "            \n",
    "            model = xgb.XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.1,\n",
    "                objective='binary:logistic',\n",
    "                eval_metric='logloss',\n",
    "                use_label_encoder=False\n",
    "            )\n",
    "            model.fit(X_col_train, y_col_train)\n",
    "            toxicity_models[col] = model\n",
    "        else:\n",
    "            print(f\"No {col} labels found in training data. Skipping this classifier.\")\n",
    "    \n",
    "    if toxicity_models:\n",
    "        models['toxicity'] = toxicity_models\n",
    "    \n",
    "    # 4. Multi-class Classification: Hate Speech\n",
    "    hate_speech_col = tasks['hate_speech']['column']\n",
    "    hate_speech_train_mask = ~df_train[hate_speech_col].isna()\n",
    "    if hate_speech_train_mask.any():\n",
    "        y_hate_speech_train = df_train.loc[hate_speech_train_mask, hate_speech_col]\n",
    "        X_hate_speech_train = X_train_combined[hate_speech_train_mask]\n",
    "        \n",
    "        # Label encode hate speech classes\n",
    "        hate_speech_encoder = LabelEncoder()\n",
    "        y_hate_speech_train_encoded = hate_speech_encoder.fit_transform(y_hate_speech_train)\n",
    "        \n",
    "        print(f\"Training hate speech classifier with {len(y_hate_speech_train)} samples\")\n",
    "        \n",
    "        hate_speech_model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            objective='multi:softprob',\n",
    "            num_class=len(np.unique(y_hate_speech_train_encoded)),\n",
    "            eval_metric='mlogloss',\n",
    "            use_label_encoder=False\n",
    "        )\n",
    "        hate_speech_model.fit(X_hate_speech_train, y_hate_speech_train_encoded)\n",
    "        models['hate_speech'] = {\n",
    "            'model': hate_speech_model,\n",
    "            'encoder': hate_speech_encoder,\n",
    "            'classes': sorted(list(hate_speech_encoder.classes_))\n",
    "        }\n",
    "    else:\n",
    "        print(\"No hate speech labels found in training data. Skipping hate speech classifier.\")\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training models...\n",
      "Training spam classifier with 1564 samples\n",
      "Training sentiment classifier with 55592 samples\n",
      "Training toxic classifier with 127656 samples\n",
      "Training severe_toxic classifier with 127656 samples\n",
      "Training obscene classifier with 127656 samples\n",
      "Training threat classifier with 127656 samples\n",
      "Training insult classifier with 127656 samples\n",
      "Training identity_hate classifier with 127656 samples\n",
      "Training hate speech classifier with 12306 samples\n"
     ]
    }
   ],
   "source": [
    "# Train all models\n",
    "print(\"\\nTraining models...\")\n",
    "models = train_and_evaluate_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsemblePredictor:\n",
    "    def __init__(self, tfidf_vectorizer, fasttext_model, models, tasks):\n",
    "        self.tfidf_vectorizer = tfidf_vectorizer\n",
    "        self.fasttext_model = fasttext_model\n",
    "        self.models = models\n",
    "        self.tasks = tasks\n",
    "    \n",
    "    def transform_features(self, texts):\n",
    "        # Create TF-IDF features\n",
    "        tfidf_features = self.tfidf_vectorizer.transform(texts)\n",
    "        \n",
    "        # Create FastText features\n",
    "        fasttext_features = np.array([get_document_vector(text, self.fasttext_model) for text in texts])\n",
    "        \n",
    "        # Combine features\n",
    "        X_combined = sp.hstack((tfidf_features, sp.csr_matrix(fasttext_features)))\n",
    "        \n",
    "        return X_combined\n",
    "    \n",
    "    def predict_with_confidence(self, texts):\n",
    "        # Transform input texts\n",
    "        X_vec = self.transform_features(texts)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. Spam prediction (if model exists)\n",
    "        if 'spam' in self.models:\n",
    "            spam_model = self.models['spam']\n",
    "            spam_probs = spam_model.predict_proba(X_vec)\n",
    "            # For binary classification, probability of class 1 is the confidence score\n",
    "            results['spam'] = {\n",
    "                'prediction': spam_model.predict(X_vec),\n",
    "                'confidence': spam_probs[:, 1]\n",
    "            }\n",
    "        \n",
    "        # 2. Sentiment prediction (if model exists)\n",
    "        if 'sentiment' in self.models:\n",
    "            sentiment_dict = self.models['sentiment']\n",
    "            sentiment_model = sentiment_dict['model']\n",
    "            sentiment_encoder = sentiment_dict['encoder']\n",
    "            sentiment_classes = sentiment_dict['classes']\n",
    "            \n",
    "            sentiment_probs = sentiment_model.predict_proba(X_vec)\n",
    "            sentiment_preds = sentiment_model.predict(X_vec)\n",
    "            sentiment_pred_labels = sentiment_encoder.inverse_transform(sentiment_preds)\n",
    "            \n",
    "            # Get confidence for each prediction\n",
    "            sentiment_conf = np.max(sentiment_probs, axis=1)\n",
    "            \n",
    "            results['sentiment'] = {\n",
    "                'prediction': sentiment_pred_labels,\n",
    "                'confidence': sentiment_conf,\n",
    "                'all_probs': {class_name: sentiment_probs[:, i] for i, class_name in enumerate(sentiment_classes)}\n",
    "            }\n",
    "        \n",
    "        # 3. Toxicity predictions (if models exist)\n",
    "        if 'toxicity' in self.models:\n",
    "            toxicity_models = self.models['toxicity']\n",
    "            toxicity_results = {}\n",
    "            \n",
    "            for col, model in toxicity_models.items():\n",
    "                probs = model.predict_proba(X_vec)\n",
    "                preds = model.predict(X_vec)\n",
    "                # For binary classification, probability of class 1 is the confidence score\n",
    "                toxicity_results[col] = {\n",
    "                    'prediction': preds,\n",
    "                    'confidence': probs[:, 1]\n",
    "                }\n",
    "            \n",
    "            results['toxicity'] = toxicity_results\n",
    "        \n",
    "        # 4. Hate Speech prediction (if model exists)\n",
    "        if 'hate_speech' in self.models:\n",
    "            hate_speech_dict = self.models['hate_speech']\n",
    "            hate_speech_model = hate_speech_dict['model']\n",
    "            hate_speech_encoder = hate_speech_dict['encoder']\n",
    "            hate_speech_classes = hate_speech_dict['classes']\n",
    "            \n",
    "            hate_speech_probs = hate_speech_model.predict_proba(X_vec)\n",
    "            hate_speech_preds = hate_speech_model.predict(X_vec)\n",
    "            hate_speech_pred_labels = hate_speech_encoder.inverse_transform(hate_speech_preds)\n",
    "            \n",
    "            # Get confidence for each prediction\n",
    "            hate_speech_conf = np.max(hate_speech_probs, axis=1)\n",
    "            \n",
    "            results['hate_speech'] = {\n",
    "                'prediction': hate_speech_pred_labels,\n",
    "                'confidence': hate_speech_conf,\n",
    "                'all_probs': {class_name: hate_speech_probs[:, i] for i, class_name in enumerate(hate_speech_classes)}\n",
    "            }\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ensemble predictor\n",
    "predictor = EnsemblePredictor(tfidf_vectorizer, fasttext_model, models, tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def save_model(predictor, filename='../src/models/fasttext_tfidf_ensemble.joblib'):\n",
    "    joblib.dump(predictor, filename)\n",
    "    print(f\"Model saved as {filename}\")\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(filename='../src/models/fasttext_tfidf_ensemble.joblib'):\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as ../src/models/fasttext_tfidf_ensemble.joblib\n"
     ]
    }
   ],
   "source": [
    "save_model(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the predictor\n",
    "def demo_prediction():\n",
    "    sample_texts = [\n",
    "        \"This video is amazing! I loved every minute of it.\",\n",
    "        \"Check out my channel for free iPhone giveaway! Click the link now!\",\n",
    "        \"You are so stupid and ugly, nobody likes you.\",\n",
    "        \"I respectfully disagree with your opinion on this matter.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nPrediction Results:\")\n",
    "    predictions = predictor.predict_with_confidence(sample_texts)\n",
    "    \n",
    "    for i, text in enumerate(sample_texts):\n",
    "        print(f\"\\nText {i+1}: {text}\")\n",
    "        \n",
    "        # Spam\n",
    "        if 'spam' in predictions:\n",
    "            spam_pred = predictions['spam']['prediction'][i]\n",
    "            spam_conf = predictions['spam']['confidence'][i]\n",
    "            print(f\"Spam: {'Yes' if spam_pred == 1 else 'No'} (confidence: {spam_conf:.4f})\")\n",
    "        \n",
    "        # Sentiment\n",
    "        if 'sentiment' in predictions:\n",
    "            sentiment_pred = predictions['sentiment']['prediction'][i]\n",
    "            sentiment_conf = predictions['sentiment']['confidence'][i]\n",
    "            print(f\"Sentiment: {sentiment_pred} (confidence: {sentiment_conf:.4f})\")\n",
    "        \n",
    "        # Toxicity\n",
    "        if 'toxicity' in predictions:\n",
    "            print(\"Toxicity:\")\n",
    "            for col in predictions['toxicity'].keys():\n",
    "                tox_pred = predictions['toxicity'][col]['prediction'][i]\n",
    "                tox_conf = predictions['toxicity'][col]['confidence'][i]\n",
    "                print(f\"  - {col}: {'Yes' if tox_pred == 1 else 'No'} (confidence: {tox_conf:.4f})\")\n",
    "        \n",
    "        # Hate Speech\n",
    "        if 'hate_speech' in predictions:\n",
    "            hate_pred = predictions['hate_speech']['prediction'][i]\n",
    "            hate_conf = predictions['hate_speech']['confidence'][i]\n",
    "            print(f\"Hate Speech: {hate_pred} (confidence: {hate_conf:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Results:\n",
      "\n",
      "Text 1: This video is amazing! I loved every minute of it.\n",
      "Spam: No (confidence: 0.0035)\n",
      "Sentiment: Positive (confidence: 0.8063)\n",
      "Toxicity:\n",
      "  - toxic: No (confidence: 0.0262)\n",
      "  - severe_toxic: No (confidence: 0.0003)\n",
      "  - obscene: No (confidence: 0.0077)\n",
      "  - threat: No (confidence: 0.0001)\n",
      "  - insult: No (confidence: 0.0046)\n",
      "  - identity_hate: No (confidence: 0.0006)\n",
      "Hate Speech: normal (confidence: 0.7429)\n",
      "\n",
      "Text 2: Check out my channel for free iPhone giveaway! Click the link now!\n",
      "Spam: Yes (confidence: 0.9988)\n",
      "Sentiment: Neutral (confidence: 0.5342)\n",
      "Toxicity:\n",
      "  - toxic: No (confidence: 0.0033)\n",
      "  - severe_toxic: No (confidence: 0.0002)\n",
      "  - obscene: No (confidence: 0.0019)\n",
      "  - threat: No (confidence: 0.0000)\n",
      "  - insult: No (confidence: 0.0014)\n",
      "  - identity_hate: No (confidence: 0.0001)\n",
      "Hate Speech: normal (confidence: 0.8477)\n",
      "\n",
      "Text 3: You are so stupid and ugly, nobody likes you.\n",
      "Spam: No (confidence: 0.0199)\n",
      "Sentiment: Negative (confidence: 0.6409)\n",
      "Toxicity:\n",
      "  - toxic: Yes (confidence: 0.9751)\n",
      "  - severe_toxic: No (confidence: 0.0224)\n",
      "  - obscene: Yes (confidence: 0.6203)\n",
      "  - threat: No (confidence: 0.0018)\n",
      "  - insult: Yes (confidence: 0.8657)\n",
      "  - identity_hate: No (confidence: 0.1025)\n",
      "Hate Speech: offensive (confidence: 0.4594)\n",
      "\n",
      "Text 4: I respectfully disagree with your opinion on this matter.\n",
      "Spam: No (confidence: 0.0305)\n",
      "Sentiment: Negative (confidence: 0.3847)\n",
      "Toxicity:\n",
      "  - toxic: No (confidence: 0.0196)\n",
      "  - severe_toxic: No (confidence: 0.0001)\n",
      "  - obscene: No (confidence: 0.0029)\n",
      "  - threat: No (confidence: 0.0001)\n",
      "  - insult: No (confidence: 0.0046)\n",
      "  - identity_hate: No (confidence: 0.0002)\n",
      "Hate Speech: normal (confidence: 0.7122)\n"
     ]
    }
   ],
   "source": [
    "# Demo the model\n",
    "demo_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "def evaluate_model():\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    \n",
    "    # Evaluate Spam Detection (if model exists)\n",
    "    if 'spam' in models:\n",
    "        spam_col = tasks['spam']['column']\n",
    "        test_mask = ~df_test[spam_col].isna()\n",
    "        if test_mask.any():\n",
    "            y_test = df_test.loc[test_mask, spam_col].astype(int)\n",
    "            X_test_filtered = X_test_combined[test_mask]\n",
    "            \n",
    "            spam_model = models['spam']\n",
    "            y_pred = spam_model.predict(X_test_filtered)\n",
    "            \n",
    "            print(\"\\nSpam Detection Evaluation:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Evaluate Sentiment Analysis (if model exists)\n",
    "    if 'sentiment' in models:\n",
    "        sentiment_col = tasks['sentiment']['column']\n",
    "        test_mask = ~df_test[sentiment_col].isna()\n",
    "        if test_mask.any():\n",
    "            y_test = df_test.loc[test_mask, sentiment_col]\n",
    "            X_test_filtered = X_test_combined[test_mask]\n",
    "            \n",
    "            sentiment_dict = models['sentiment']\n",
    "            sentiment_model = sentiment_dict['model']\n",
    "            sentiment_encoder = sentiment_dict['encoder']\n",
    "            \n",
    "            # Encode test labels using the same encoder\n",
    "            try:\n",
    "                y_test_encoded = sentiment_encoder.transform(y_test)\n",
    "                y_pred = sentiment_model.predict(X_test_filtered)\n",
    "                \n",
    "                print(\"\\nSentiment Analysis Evaluation:\")\n",
    "                print(classification_report(y_test_encoded, y_pred))\n",
    "            except ValueError as e:\n",
    "                print(f\"\\nError in sentiment evaluation: {e}\")\n",
    "                print(\"This might be due to new classes in test data that weren't in training data\")\n",
    "    \n",
    "    # Evaluate Toxicity Detection (if models exist)\n",
    "    if 'toxicity' in models:\n",
    "        toxicity_models = models['toxicity']\n",
    "        for col, model in toxicity_models.items():\n",
    "            test_mask = ~df_test[col].isna()\n",
    "            if test_mask.any():\n",
    "                y_test = df_test.loc[test_mask, col].astype(int)\n",
    "                X_test_filtered = X_test_combined[test_mask]\n",
    "                \n",
    "                y_pred = model.predict(X_test_filtered)\n",
    "                \n",
    "                print(f\"\\n{col.capitalize()} Classification Evaluation:\")\n",
    "                print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Evaluate Hate Speech Detection (if model exists)\n",
    "    if 'hate_speech' in models:\n",
    "        hate_speech_col = tasks['hate_speech']['column']\n",
    "        test_mask = ~df_test[hate_speech_col].isna()\n",
    "        if test_mask.any():\n",
    "            y_test = df_test.loc[test_mask, hate_speech_col]\n",
    "            X_test_filtered = X_test_combined[test_mask]\n",
    "            \n",
    "            hate_speech_dict = models['hate_speech']\n",
    "            hate_speech_model = hate_speech_dict['model']\n",
    "            hate_speech_encoder = hate_speech_dict['encoder']\n",
    "            \n",
    "            # Encode test labels using the same encoder\n",
    "            try:\n",
    "                y_test_encoded = hate_speech_encoder.transform(y_test)\n",
    "                y_pred = hate_speech_model.predict(X_test_filtered)\n",
    "                \n",
    "                print(\"\\nHate Speech Detection Evaluation:\")\n",
    "                print(classification_report(y_test_encoded, y_pred))\n",
    "            except ValueError as e:\n",
    "                print(f\"\\nError in hate speech evaluation: {e}\")\n",
    "                print(\"This might be due to new classes in test data that weren't in training data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation:\n",
      "\n",
      "Spam Detection Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94       176\n",
      "           1       0.96      0.94      0.95       216\n",
      "\n",
      "    accuracy                           0.94       392\n",
      "   macro avg       0.94      0.94      0.94       392\n",
      "weighted avg       0.94      0.94      0.94       392\n",
      "\n",
      "\n",
      "Sentiment Analysis Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.31      0.40      2474\n",
      "           1       0.63      0.79      0.70      4260\n",
      "           2       0.62      0.54      0.58      3397\n",
      "           3       0.59      0.68      0.63      3768\n",
      "\n",
      "    accuracy                           0.61     13899\n",
      "   macro avg       0.60      0.58      0.58     13899\n",
      "weighted avg       0.61      0.61      0.60     13899\n",
      "\n",
      "\n",
      "Toxic Classification Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     28859\n",
      "           1       0.87      0.64      0.74      3056\n",
      "\n",
      "    accuracy                           0.96     31915\n",
      "   macro avg       0.92      0.81      0.86     31915\n",
      "weighted avg       0.95      0.96      0.95     31915\n",
      "\n",
      "\n",
      "Severe_toxic Classification Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     31594\n",
      "           1       0.47      0.30      0.36       321\n",
      "\n",
      "    accuracy                           0.99     31915\n",
      "   macro avg       0.73      0.65      0.68     31915\n",
      "weighted avg       0.99      0.99      0.99     31915\n",
      "\n",
      "\n",
      "Obscene Classification Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     30200\n",
      "           1       0.86      0.69      0.77      1715\n",
      "\n",
      "    accuracy                           0.98     31915\n",
      "   macro avg       0.92      0.84      0.88     31915\n",
      "weighted avg       0.98      0.98      0.98     31915\n",
      "\n",
      "\n",
      "Threat Classification Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     31841\n",
      "           1       0.52      0.22      0.30        74\n",
      "\n",
      "    accuracy                           1.00     31915\n",
      "   macro avg       0.76      0.61      0.65     31915\n",
      "weighted avg       1.00      1.00      1.00     31915\n",
      "\n",
      "\n",
      "Insult Classification Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     30301\n",
      "           1       0.79      0.58      0.67      1614\n",
      "\n",
      "    accuracy                           0.97     31915\n",
      "   macro avg       0.89      0.79      0.83     31915\n",
      "weighted avg       0.97      0.97      0.97     31915\n",
      "\n",
      "\n",
      "Identity_hate Classification Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     31621\n",
      "           1       0.65      0.26      0.37       294\n",
      "\n",
      "    accuracy                           0.99     31915\n",
      "   macro avg       0.82      0.63      0.68     31915\n",
      "weighted avg       0.99      0.99      0.99     31915\n",
      "\n",
      "\n",
      "Hate Speech Detection Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.67      0.71       963\n",
      "           1       0.61      0.82      0.70      1243\n",
      "           2       0.59      0.35      0.44       871\n",
      "\n",
      "    accuracy                           0.64      3077\n",
      "   macro avg       0.65      0.62      0.62      3077\n",
      "weighted avg       0.65      0.64      0.63      3077\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to use the model for predictions on new data\n",
    "def predict_youtube_comments(comments, model_path='fasttext_tfidf_ensemble.joblib'):\n",
    "    \"\"\"\n",
    "    Predict labels for new YouTube comments\n",
    "    \n",
    "    Args:\n",
    "        comments: List of strings containing YouTube comments\n",
    "        model_path: Path to the saved model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with predictions and confidence scores\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    predictor = load_model(model_path)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = predictor.predict_with_confidence(comments)\n",
    "    \n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yca-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
