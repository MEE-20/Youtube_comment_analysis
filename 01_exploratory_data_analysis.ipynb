{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - YouTube Comment Analysis Project\n",
    "\n",
    "This notebook explores three different datasets:\n",
    "1. YouTube Spam Comments\n",
    "2. Twitter Data Analysis\n",
    "3. Toxic Comment Analysis\n",
    "\n",
    "We'll analyze these datasets to understand their structure and characteristics to inform our YouTube comment analysis project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Project root directory\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "DATASETS_DIR = PROJECT_ROOT / 'datasets' / 'raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. YouTube Spam Comments Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Read and combine all YouTube spam datasets\n",
    "youtube_files = list(DATASETS_DIR.glob('youtube_spam_data/*.csv'))\n",
    "youtube_dfs = []\n",
    "\n",
    "for file in youtube_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df['source'] = file.stem  # Add source file name\n",
    "    youtube_dfs.append(df)\n",
    "\n",
    "youtube_df = pd.concat(youtube_dfs, ignore_index=True)\n",
    "\n",
    "# Display basic information\n",
    "print(\"YouTube Spam Dataset Info:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total number of comments: {len(youtube_df)}\")\n",
    "print(\"\\nColumns:\")\n",
    "youtube_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic statistics of YouTube spam dataset\n",
    "print(\"Distribution of comments across videos:\")\n",
    "print(youtube_df['source'].value_counts())\n",
    "\n",
    "# Check class distribution (spam vs non-spam)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=youtube_df, x='CLASS')\n",
    "plt.title('Distribution of Spam vs Non-Spam Comments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Twitter Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Read Twitter datasets\n",
    "twitter_train = pd.read_csv(DATASETS_DIR / 'twitter_data/twitter_training.csv')\n",
    "twitter_val = pd.read_csv(DATASETS_DIR / 'twitter_data/twitter_validation.csv')\n",
    "\n",
    "print(\"Twitter Dataset Info:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Training set size: {len(twitter_train)}\")\n",
    "print(f\"Validation set size: {len(twitter_val)}\")\n",
    "print(\"\\nColumns:\")\n",
    "twitter_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Toxic Comment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Read Toxic comment datasets\n",
    "toxic_train = pd.read_csv(DATASETS_DIR / 'toxic_comment_data/train.csv')\n",
    "toxic_test = pd.read_csv(DATASETS_DIR / 'toxic_comment_data/test.csv')\n",
    "toxic_test_labels = pd.read_csv(DATASETS_DIR / 'toxic_comment_data/test_labels.csv')\n",
    "\n",
    "print(\"Toxic Comment Dataset Info:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Training set size: {len(toxic_train)}\")\n",
    "print(f\"Test set size: {len(toxic_test)}\")\n",
    "print(\"\\nColumns:\")\n",
    "toxic_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Length Analysis Across Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def add_text_length(df, text_column):\n",
    "    df['text_length'] = df[text_column].str.len()\n",
    "    return df\n",
    "\n",
    "# Add text length to each dataset\n",
    "youtube_df = add_text_length(youtube_df, 'CONTENT')\n",
    "twitter_train = add_text_length(twitter_train, 'text')  # Adjust column name if different\n",
    "toxic_train = add_text_length(toxic_train, 'comment_text')\n",
    "\n",
    "# Plot text length distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "sns.histplot(data=youtube_df, x='text_length', bins=50)\n",
    "plt.title('YouTube Comments Length')\n",
    "plt.xlabel('Length')\n",
    "\n",
    "plt.subplot(132)\n",
    "sns.histplot(data=twitter_train, x='text_length', bins=50)\n",
    "plt.title('Twitter Text Length')\n",
    "plt.xlabel('Length')\n",
    "\n",
    "plt.subplot(133)\n",
    "sns.histplot(data=toxic_train, x='text_length', bins=50)\n",
    "plt.title('Toxic Comments Length')\n",
    "plt.xlabel('Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Words and Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_common_words(text_series, n=20):\n",
    "    # Combine all text\n",
    "    text = ' '.join(text_series.astype(str))\n",
    "    # Convert to lowercase and split into words\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    # Get most common words\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "# Analyze common words in each dataset\n",
    "print(\"Most common words in YouTube comments:\")\n",
    "print(get_common_words(youtube_df['CONTENT']))\n",
    "\n",
    "print(\"\\nMost common words in Twitter data:\")\n",
    "print(get_common_words(twitter_train['text']))  # Adjust column name if different\n",
    "\n",
    "print(\"\\nMost common words in Toxic comments:\")\n",
    "print(get_common_words(toxic_train['comment_text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
